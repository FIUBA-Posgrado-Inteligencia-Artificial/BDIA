{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso que se desee persistir la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chromadb.config import Settings\n",
    "# chroma_client = chromadb.Client(\n",
    "#     Settings(\n",
    "#         chroma_db_impl=\"duckdb+parquet\",\n",
    "#         persist_directory='my_personal_vector_db',\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombramos la colección que vamos a utilizar como \"my_news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"my_news\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chroma_client.list_collections()** retorna una lista con información acerca de las colecciones vigentes dentro de la base persistida\n",
    "\n",
    "En caso que la colección ya exista, y que exista una con el mismo nombre que aquella que intentamos crea, va a ser eliminada para comenzar el proceso nuevamente partiendo desde el inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "else:\n",
    "    print(f\"Creating collection: '{collection_name}'\")\n",
    "    collection = chroma_client.create_collection(name=collection_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para embeddings customizados, es necesario crear una nueva **función de embedding** que permita procesar texto\n",
    "\n",
    "```python\n",
    "    collection = chroma_client.create_collection(name=\"my_collection\", embedding_function=emb_fn)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(\"labelled_newscatcher_coloured.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_subset = pdf.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones CRUD\n",
    "Sobre esta base de datos vamos a utilizar operaciones CRUD (Create, Read, Update & Delete) con sintaxis similar a MongoDB\n",
    "\n",
    "Al momento de insertar documentos sobre una base de datos de vectores, se insertan los documentos que van a ser vectorizados, en conjunto con los IDs que van a ser usados para identificar dichos documentos y la metadata asociada a los mismos\n",
    "\n",
    "Así como pueden insertarse documentos, Chromadb soporta la inserción de **embeddings** de forma directa sin necesidad de especificar algún documento, esto resulta útil para realizar búsquedas con texto sobre bases de datos de imágenes con modelos como [CLIP](https://huggingface.co/openai/clip-vit-large-patch14) capaces de manejar ambos tipos de información (texto y visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=pdf_subset[\"title\"][:100].to_list(),\n",
    "    metadatas=[{\"topic\": topic} for topic in pdf_subset[\"topic\"][:100].tolist()],\n",
    "    ids=[f\"id{x}\" for x in range(100)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"space\"],\n",
    "    # query_texts=[\"espacio\"],\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"bombs\"],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(\n",
    "    query_texts=[\"space\"],\n",
    "    # en el caso de operaciones de filtrado usando  \"where\", \n",
    "    # pueden darse operadores $and, $or, $ge, etc, de la misma forma que se dieron con MongoDB\n",
    "    where={\"topic\": \"SCIENCE\"}, \n",
    "    n_results=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borramos el primer elemento de la colección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.delete(\n",
    "    ids=[\"id0\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que ya no se encuentra disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    ids=[\"id0\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora realizamos un ejemplo de actualización de un documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    ids=[\"id2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el documento 2, vamos a cambiar su tópico de \"SCIENCE\" a \"TECHNOLOGY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.update(\n",
    "    ids=[\"id2\"],\n",
    "    metadatas={\"topic\": \"TECHNOLOGY\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que el mismo haya cambiado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get(\n",
    "    ids=[\"id2\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se va a armar un pipeline de Q&A sencillo utilizando como fuente de datos, la base de datos que acabamos de crear con ChromaDB.\n",
    "\n",
    "El hito es poder proveer de contexto a algún modelo generativo de lenguaje (en este caso en particular a GPT2) tratando de eficientizar el performance del modelo y tratar de acortar la ventana de contexto necesaria para que el modelo pueda funcionar correctamente\n",
    "\n",
    "Recordar que GPT2 es una versión gratuita y un modelo antiguo de GPT4, no es de esperar que cuente con el mismo performance que modelos como GPT3 en adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # cache_dir='cache'\n",
    ")\n",
    "\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # cache_dir='cache',\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"space\"],\n",
    "    # query_texts=[\"espacio\"],\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What's the latest news on space development?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos el contexto necesario para responder la pregunta del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \" \".join([f\"\\n#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\"\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_response = pipe(prompt_template)\n",
    "print(lm_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para más información sobre como incorporar GPT4 y armar un chatbot de Q&A, visitar [Embeddings con OPEN AI](https://docs.trychroma.com/embeddings) para comprender cómo puede integrarse ChromaDB "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
